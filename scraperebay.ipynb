{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import requests\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageOps\n",
    "from io import BytesIO\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = [\n",
    "    \"bicycle\",\n",
    "    \"bus\",\n",
    "    \"truck\",\n",
    "    \"boat\",\n",
    "    \"parking meter\",\n",
    "    \"bench\",\n",
    "    \"backpack\",\n",
    "    \"umbrella\",\n",
    "    \"handbag\",\n",
    "    \"tie\",\n",
    "    \"suitcase\",\n",
    "    \"frisbee\",\n",
    "    \"skis\",\n",
    "    \"snowboard\",\n",
    "    \"sports ball\",\n",
    "    \"kite\",\n",
    "    \"baseball bat\",\n",
    "    \"baseball glove\",\n",
    "    \"skateboard\",\n",
    "    \"laptop\",\n",
    "    \"mouse\",\n",
    "    \"remote\",\n",
    "    \"keyboard\",\n",
    "    \"cell phone\",\n",
    "    \"microwave\",\n",
    "    \"oven\",\n",
    "    \"toaster\",\n",
    "    \"sink\",\n",
    "    \"book\",\n",
    "    \"clock\",\n",
    "    \"chair\",\n",
    "    \"sofa\",\n",
    "    \"bed\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_LINK_FILE = 'image_links.csv'\n",
    "CSV_METADATA_FILE = 'image_metadata.csv'\n",
    "OUTPUT_DIR = 'scraped_images'\n",
    "MAX_IMAGES_PER_CATEGORY = 500\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)  \n",
    "for category in CATEGORIES:\n",
    "    category_dir = os.path.join(OUTPUT_DIR, category)\n",
    "    os.makedirs(category_dir, exist_ok=True)  # Create the category folder if it doesn't exist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(csv_file, row):\n",
    "    if not os.path.exists(csv_file):\n",
    "        with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "            csv.writer(file).writerow(['category', 'image_url', 'download_time', 'file_path'])  # Write header\n",
    "\n",
    "    with open(csv_file, mode='r+', newline='', encoding='utf-8') as file:\n",
    "        if row not in csv.reader(file):\n",
    "            csv.writer(file).writerow(row)\n",
    "        else:\n",
    "            print(f\"Row already exists in CSV: {row}\")\n",
    "\n",
    "def write_links_to_csv(csv_path, data):\n",
    "    os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
    "    write_header = not os.path.exists(csv_path)\n",
    "    with open(csv_path, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if write_header:\n",
    "            writer.writerow(['category', 'url', 'source', 'timestamp'])\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll_to_bottom(driver, max_scrolls=15):\n",
    "    \"\"\"Scrolls to the bottom of the page to load more content.\"\"\"\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    scroll_count = 0\n",
    "\n",
    "    while scroll_count < max_scrolls:  \n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  \n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            print(\"Reached the bottom of the page or no new content to load.\")\n",
    "            break\n",
    "\n",
    "        print(f\"Scrolled {scroll_count + 1} times\")\n",
    "        last_height = new_height\n",
    "        scroll_count += 1\n",
    "\n",
    "def get_image_urls(query, classes, location, source):\n",
    "    try:\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--headless=new')\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "        search_url = f\"https://www.ebay.com/sch/i.html?_from=R40&_trksid=p2334524.m570.l1313&_nkw={query}&_sacat=0&LH_TitleDesc=0&_osacat=0&_odkw={query}\"\n",
    "        print(f\"Scraping URL: {search_url}\")\n",
    "        driver.get(search_url)\n",
    "\n",
    "        scroll_to_bottom(driver)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        driver.quit()\n",
    "\n",
    "        image_urls = []\n",
    "        elements = soup.findAll(attrs={\"class\": classes})\n",
    "        for element in elements:\n",
    "            url = element.find(location)\n",
    "            if url and url.get(source) and url.get(source) not in image_urls:\n",
    "                image_urls.append(url.get(source))\n",
    "\n",
    "        print(f\"Found {len(image_urls)} image URLs.\")\n",
    "        return image_urls\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape image URLs: {str(e)}\")\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aspect_ratio(image):\n",
    "    width, height = image.size\n",
    "    aspect_ratio = width / height\n",
    "    return aspect_ratio\n",
    "\n",
    "def find_closest_aspect_ratio(image):\n",
    "    aspect_ratios = {\n",
    "        \"1:1\": 1.0,\n",
    "        \"4:3\": 4 / 3,\n",
    "        \"16:9\": 16 / 9\n",
    "    }\n",
    "\n",
    "    original_aspect_ratio = get_aspect_ratio(image)\n",
    "    closest_ratio = min(aspect_ratios, key=lambda ratio: abs(aspect_ratios[ratio] - original_aspect_ratio))\n",
    "    return closest_ratio\n",
    "\n",
    "def resize_image(image, target_size):\n",
    "    image = ImageOps.exif_transpose(image)\n",
    "    new_image = Image.new(\"RGB\", target_size, (255, 255, 255))\n",
    "    image.thumbnail(target_size)\n",
    "    x_offset = (target_size[0] - image.size[0]) // 2\n",
    "    y_offset = (target_size[1] - image.size[1]) // 2\n",
    "    new_image.paste(image, (x_offset, y_offset))\n",
    "    return new_image\n",
    "\n",
    "def resize_and_save(image, file_path):\n",
    "    closest_ratio = find_closest_aspect_ratio(image)\n",
    "    if closest_ratio == \"1:1\":\n",
    "        resized_img = resize_image(image, (640, 640))\n",
    "    elif closest_ratio == \"4:3\":\n",
    "        resized_img = resize_image(image, (1024, 768))\n",
    "    elif closest_ratio == \"16:9\":\n",
    "        resized_img = resize_image(image, (1280, 720))\n",
    "    resized_img.save(file_path.replace('.jpg', f'_{closest_ratio.replace(\":\", \"-\")}.jpg'))\n",
    "\n",
    "def download_image_from_csv_row(category, url, output_dir):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            image_content = response.content\n",
    "            file_hash = hashlib.md5(image_content).hexdigest()\n",
    "            image = Image.open(BytesIO(image_content))\n",
    "            image_format = image.format\n",
    "\n",
    "            if image_format == 'PNG':\n",
    "                file_ext = 'png'\n",
    "            elif image_format in ['JPEG', 'JPG', 'WEBP']:\n",
    "                file_ext = 'jpg'\n",
    "            else:\n",
    "                print(f\"Unsupported format: {image_format}\")\n",
    "                return\n",
    "\n",
    "            category_path = os.path.join(output_dir, category)\n",
    "            os.makedirs(category_path, exist_ok=True)\n",
    "            file_path = os.path.join(category_path, f\"{file_hash}.{file_ext}\")\n",
    "\n",
    "            if not os.path.exists(file_path):\n",
    "                image.save(file_path, image_format)\n",
    "                resize_and_save(image, file_path)\n",
    "                print(f\"Saved image: {file_path}\")\n",
    "\n",
    "                with open(CSV_METADATA_FILE, mode='a', newline='') as metafile:\n",
    "                    meta_writer = csv.writer(metafile)\n",
    "                    if os.stat(CSV_METADATA_FILE).st_size == 0:\n",
    "                        meta_writer.writerow(['category', 'url', 'download_time', 'file_path'])\n",
    "                    meta_writer.writerow([category, url, datetime.now(), file_path])\n",
    "            else:\n",
    "                print(f\"Image already exists: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {url}: {e}\")\n",
    "\n",
    "def download_images_from_csv(csv_path=CSV_LINK_FILE, output_dir=OUTPUT_DIR):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    for _, row in df.iterrows():\n",
    "        category = row['category']\n",
    "        url = row['url']\n",
    "        download_image_from_csv_row(category, url, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_images():\n",
    "    for category in CATEGORIES:\n",
    "        query = category.replace(' ', '+')\n",
    "        print(f'Scraping images for: {category}')\n",
    "        image_urls = get_image_urls(query, \"s-item__image-wrapper image-treatment\", \"img\", \"src\")\n",
    "        timestamp = datetime.now()\n",
    "        link_rows = [(category, url, \"ebay\", timestamp) for url in image_urls[:MAX_IMAGES_PER_CATEGORY]]\n",
    "        write_links_to_csv(category, CSV_LINK_FILE, link_rows)\n",
    "        print(f\"Saved {len(link_rows)} links for {category}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping images for: bicycle\n",
      "Scraping URL: https://www.ebay.com/sch/i.html?_from=R40&_trksid=p2334524.m570.l1313&_nkw=bicycle&_sacat=0&LH_TitleDesc=0&_osacat=0&_odkw=bicycle\n",
      "Scrolled 1 times\n",
      "Scrolled 2 times\n",
      "Reached the bottom of the page or no new content to load.\n",
      "Found 72 image URLs.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m scrape_images()\n",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m, in \u001b[0;36mscrape_images\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m timestamp \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m      7\u001b[0m link_rows \u001b[38;5;241m=\u001b[39m [(category, url, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mebay\u001b[39m\u001b[38;5;124m\"\u001b[39m, timestamp) \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m image_urls[:MAX_IMAGES_PER_CATEGORY]]\n\u001b[1;32m----> 8\u001b[0m write_links_to_csv(CSV_LINK_FILE, link_rows)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(link_rows)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m links for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 13\u001b[0m, in \u001b[0;36mwrite_links_to_csv\u001b[1;34m(csv_path, data)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrite_links_to_csv\u001b[39m(csv_path, data):\n\u001b[1;32m---> 13\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(csv_path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m     write_header \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(csv_path)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(csv_path, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "File \u001b[1;32m<frozen os>:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: ''"
     ]
    }
   ],
   "source": [
    "scrape_images()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_images_from_csv()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
